{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b434b50",
   "metadata": {},
   "source": [
    "# Projet Representation Learning for NLP\n",
    "\n",
    "M2 MIASHS - Université de Lyon - Janvier 2026\n",
    "\n",
    "Luan DECHERY – Mathis DERENNE – Enrique NASSEH FREITAS\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7046bb",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Depuis la publication de l'article *\"Attention Is All You Need\"* (Vaswani et al., 2017), le traitement du langage naturel (NLP) a connu une transformation majeure. Les approches précédentes, comme **Bag-of-Words** ou **TF-IDF**, représentaient les mots par comptage de fréquence, tandis que **Word2Vec** avait permis une avancée significative en créant des embeddings capturant certaines relations sémantiques. Cependant, ces modèles présentaient une limitation fondamentale : l'absence de prise en compte du **contexte**.\n",
    "\n",
    "En effet, les embeddings statiques ne capturaient ni la **position des mots** (\"Pierre mange une pizza\" diffère de \"Une pizza mange Pierre\"), ni la **polysémie** (le mot \"modèle\" désigne un concept mathématique ou une profession selon le contexte). Pour résoudre ces limitations, Vaswani et al. ont introduit l'architecture **Transformer**, reposant sur un mécanisme de **self-attention** qui permet de capturer les dépendances entre tous les mots d'une séquence. Contrairement aux approches séquentielles (RNN, LSTM), cette architecture permet une parallélisation efficace des calculs sur GPU.\n",
    "\n",
    "Le Transformer original comprend deux composants : un **encodeur** qui transforme le texte en représentations vectorielles contextualisées, et un **décodeur** qui génère du texte à partir de ces représentations via un mécanisme de cross-attention. Des architectures dérivées se sont ensuite spécialisées : **BERT** (Devlin et al., 2019) utilise une architecture *encoder-only* avec une attention bidirectionnelle pour des tâches de compréhension (classification, NER), tandis que **GPT** (Radford et al., 2018) utilise une architecture *decoder-only* avec du masked self-attention pour des tâches de génération (agents conversationnels comme ChatGPT ou Claude).\n",
    "\n",
    "Ces modèles, souvent appelés **LLM** (Large Language Models), suivent généralement deux phases d'entraînement. Le **pré-entraînement** permet au modèle d'apprendre le langage de manière auto-supervisée sur de larges corpus, produisant un *foundation model*. Le **fine-tuning** adapte ensuite ce modèle à une tâche spécifique. Parmi les méthodes récentes, **LoRA** (Hu et al., 2021) permet un ajustement efficace en ne modifiant qu'une fraction des paramètres (~0.3%), rendant le fine-tuning accessible avec des ressources limitées.\n",
    "\n",
    "Dans ce projet, nous abordons la tâche d'**inférence en langage naturel (NLI)** en français, qui consiste à déterminer la relation sémantique entre deux phrases : neutralité, contradiction ou conséquence (*entailment*). \n",
    "\n",
    "| Relation | Description | Exemple |\n",
    "|----------|-------------|----------|\n",
    "| **Entailment** (Conséquence) | L'hypothèse découle logiquement de la prémisse | P: \"Le chat dort sur le canapé\" → H: \"Un animal est sur un meuble\" |\n",
    "| **Contradiction** | L'hypothèse contredit la prémisse | P: \"Le chat dort sur le canapé\" → H: \"Le chat court dehors\" |\n",
    "| **Neutral** | Aucune relation logique claire | P: \"Le chat dort sur le canapé\" → H: \"Le chat aime le poisson\" |\n",
    "\n",
    "Le jeu de données est divisé en une ensemble d'entraînement contenant 5 010 exemples et un ensemble d'évaluation avec 2 490 exemples, équilibrés entre les trois classes.\n",
    "\n",
    "Cette tâche est fondamentale en NLP car elle évalue la capacité d'un modèle à raisonner sur le sens des phrases. \n",
    "\n",
    "Ce projet implémente et compare plusieurs approches pour réaliser cette tâche : \n",
    "- **Apprentissage en contexte** avec un LLM pré-entraîné\n",
    "- **Fine-tuning d'un encodeur** (tâche de classification multi-classes)\n",
    "- **Fine-tuning d'un LLM pré-entraîné**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6a4596",
   "metadata": {},
   "source": [
    "### Apprentissage en contexte\n",
    "\n",
    "L'apprentissage en contexte consiste à interroger directement un modèle de langage pré-entrainé. Cette approche est la plus simple et n'ajuste pas les poids du modèle. On utilise comme modèle de base **Llama 3.2 3B Instruct**.\n",
    "\n",
    "Plusieurs approches sont testés et comparés :\n",
    "- zero-shot: le modèle doit répondre sans exemple préalable\n",
    "- few-shot: le modèle reçoit quelques exemples annotés en contexte avant de répondre\n",
    "- chaîne de raisonnement (Chain Of Thought (CoT)): le modèle est encouragé à expliquer son raisonnement avant de donner une réponse finale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3e5be4",
   "metadata": {},
   "source": [
    "### Fine-tuning d'un encodeur\n",
    "\n",
    "L'utilisation d'une architecture de type **encodeur** est particulièrement adaptée aux tâches de classification de séquences comme la NLI. Contrairement aux modèles génératifs, l'encodeur traite la séquence de manière bidirectionnelle, permettant à chaque mot d'intégrer l'information de l'ensemble de la prémisse et de l'hypothèse dès les premières couches du modèle.\n",
    "\n",
    "Pour ce projet, nous nous appuyons sur **CamemBERT 2.0** et **CamemBERTa 2.0**, des évolutions de l'état de l'art pour la langue française.\n",
    "\n",
    "* **Mécanisme de Classification :** Le modèle traite la paire de phrases comme une entrée unique. La représentation vectorielle du jeton spécial `[CLS]` (ou `<s>`), extraite de la dernière couche cachée, sert de résumé sémantique de la relation entre les deux phrases. Cette représentation est ensuite envoyée vers une tête de classification (couche dense) qui projette le vecteur vers les trois classes cibles : *entailment*, *neutral*, et *contradiction*.\n",
    "\n",
    "* **Optimisation par LoRA (Low-Rank Adaptation) :** Afin de limiter le coût computationnel et de prévenir l'oubli catastrophique, nous utilisons la méthode **LoRA**. Au lieu de mettre à jour la totalité des paramètres du modèle, nous injectons des matrices de bas rang dans les couches d'attention (spécifiquement sur les modules *query* et *value*). Mathématiquement, pour une matrice de poids initiale $W_0 \\in \\mathbb{R}^{d \\times k}$, la mise à jour est modélisée par :\n",
    "$$W = W_0 + \\Delta W = W_0 + BA$$\n",
    "où $B \\in \\mathbb{R}^{d \\times r}$ et $A \\in \\mathbb{R}^{r \\times k}$ sont des matrices de rang $r \\ll d$.\n",
    "\n",
    "* **Entraînement :** Le processus repose sur un apprentissage supervisé classique avec une fonction de perte de type **Cross-Entropy**, ajustant uniquement les adaptateurs LoRA et la tête de classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0e9dc1",
   "metadata": {},
   "source": [
    "### Fine-tuning d'un LLM\n",
    "\n",
    "Le fine-tuning d'un modèle de type **décodeur** (LLM) transforme radicalement la nature de la tâche. Ici, l'inférence n'est plus traitée comme un problème de classification pur, mais comme une tâche de **génération de texte sous contrainte**. Nous utilisons pour cela la famille de modèles **Llama 3.2 1B**.\n",
    "\n",
    "Cette approche se distingue par les étapes suivantes :\n",
    "\n",
    "* **Formatage des données (Prompting) :** Chaque exemple est converti en une instruction textuelle structurée. Le modèle doit apprendre à prédire le prochain jeton (token) qui correspond à la classe correcte.\n",
    "> **Exemple de prompt :**\n",
    "> \"Détermine la relation logique entre les deux phrases suivantes. Réponds uniquement par : conséquence, neutralité ou contradiction.\n",
    "> Prémisse : [Phrase A] Hypothèse : [Phrase B] Relation :\"\n",
    "\n",
    "* **Apprentissage par Causal Language Modeling (CLM) :** Le modèle est entraîné à maximiser la probabilité de générer l'étiquette correcte à la suite du prompt. Contrairement à l'encodeur, le calcul de la perte se fait sur la partie \"réponse\" de la séquence générée.\n",
    "* **Adaptation avec LoRA :** À l'instar de l'encodeur, LoRA est appliqué pour permettre un entraînement efficace sur des GPUs grand public. L'ajustement porte sur les poids du mécanisme d'auto-attention afin d'aligner la génération du modèle sur le vocabulaire spécifique des labels de notre dataset.\n",
    "* **Inférence et Post-traitement :** Lors de l'évaluation, nous demandons au modèle de générer un jeton. Un mécanisme de mapping est ensuite appliqué pour convertir la chaîne de caractères générée (ex: \"conséquence\") en label numérique afin de calculer les métriques de performance (Accuracy, F1-score)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a64896",
   "metadata": {},
   "source": [
    "### Comparaison Encodeur vs Décodeur\n",
    "\n",
    "L'architecture Transformer repose sur un mécanisme d'attention qui calcule les relations entre tous les mots d'une séquence :\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "où $Q$, $K$ et $V$ sont des projections linéaires de l'entrée. La différence entre encodeur et décodeur réside dans les positions que chaque mot peut observer.\n",
    "\n",
    "L'**encodeur** utilise une attention bidirectionnelle : chaque mot accède à tous les autres mots de la séquence. Cette architecture est adaptée aux tâches de compréhension (classification, NLI). Le pré-entraînement s'effectue par *Masked Language Modeling* où le modèle prédit des mots masqués. BERT et CamemBERT illustrent cette approche.\n",
    "\n",
    "Le **décodeur** utilise une attention causale : chaque mot ne voit que les mots précédents. Cette contrainte simule la génération de texte. Le pré-entraînement repose sur la prédiction du mot suivant. GPT et Llama adoptent cette architecture, mais peuvent être adaptés à la classification via des prompts.\n",
    "\n",
    "| | **Encodeur** | **Décodeur** |\n",
    "|---|---|---|\n",
    "| Attention | Bidirectionnelle | Causale |\n",
    "| Pré-entraînement | Masked LM | Next Token Prediction |\n",
    "| Tâches | Classification, NLI | Génération, Chat |\n",
    "| Exemples | BERT, CamemBERT | GPT, Llama |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a233dfd1",
   "metadata": {},
   "source": [
    "### Méthode LoRA\n",
    "\n",
    "Le fine-tuning classique modifie tous les paramètres du modèle, ce qui pose des problèmes de mémoire et de temps pour un modèle d'1 milliard de paramètres. **LoRA** (Low-Rank Adaptation) résout ce problème en ajoutant une modification de rang faible :\n",
    "\n",
    "$$h = Wx + BAx$$\n",
    "\n",
    "où $W$ est la matrice originale (gelée), $B \\in \\mathbb{R}^{d \\times r}$ et $A \\in \\mathbb{R}^{r \\times d}$ avec $r \\ll d$. Seules $A$ et $B$ sont entraînées.\n",
    "\n",
    "Pour une matrice $4096 \\times 4096$ avec $r = 16$ : le fine-tuning classique entraîne 16.7M paramètres, LoRA n'en entraîne que 131K, soit une **réduction de 99.2%**.\n",
    "\n",
    "| **Paramètre** | **Valeur** | **Rôle** |\n",
    "|---|---|---|\n",
    "| $r$ | 16 | Rang des matrices A et B |\n",
    "| $\\alpha$ | 32 | Facteur de mise à l'échelle |\n",
    "| Modules | q, k, v, o_proj | Projections d'attention |\n",
    "| Dropout | 0.05 | Régularisation |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
